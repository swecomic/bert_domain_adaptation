{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_domain_adaptation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA5K3JXsJ_X4",
        "outputId": "0621083e-f908-4bca-9f79-cf024c6ff592"
      },
      "source": [
        "### BERT run_mlm.py 다운로드\n",
        "!wget \"https://raw.githubusercontent.com/huggingface/transformers/4c32f9f26e6a84f0d9843fec8757e6ce640bb44e/examples/language-modeling/run_mlm.py\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-20 22:59:21--  https://raw.githubusercontent.com/huggingface/transformers/4c32f9f26e6a84f0d9843fec8757e6ce640bb44e/examples/language-modeling/run_mlm.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20496 (20K) [text/plain]\n",
            "Saving to: ‘run_mlm.py’\n",
            "\n",
            "run_mlm.py          100%[===================>]  20.02K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-07-20 22:59:21 (31.5 MB/s) - ‘run_mlm.py’ saved [20496/20496]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FCnrNXVxKQHM",
        "outputId": "56525df6-134b-4d9b-f582-15b0d78b53c7"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-1.9.0-py3-none-any.whl (262 kB)\n",
            "\u001b[K     |████████████████████████████████| 262 kB 5.4 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<0.1.0\n",
            "  Downloading huggingface_hub-0.0.14-py3-none-any.whl (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.6.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting fsspec>=2021.05.0\n",
            "  Downloading fsspec-2021.7.0-py3-none-any.whl (118 kB)\n",
            "\u001b[K     |████████████████████████████████| 118 kB 10.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 10.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: xxhash, huggingface-hub, fsspec, datasets\n",
            "Successfully installed datasets-1.9.0 fsspec-2021.7.0 huggingface-hub-0.0.14 xxhash-2.0.2\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.8.2-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers) (3.13)\n",
            "Collecting huggingface-hub==0.0.12\n",
            "  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 63.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.0.14\n",
            "    Uninstalling huggingface-hub-0.0.14:\n",
            "      Successfully uninstalled huggingface-hub-0.0.14\n",
            "Successfully installed huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.8.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C_ROW4iKRUf",
        "outputId": "5a73731a-1581-4998-fa34-fd1acf059e06"
      },
      "source": [
        "### BERT MLM pretraining\n",
        "### kykim/bert-kor-base 모델을 불러와 새로운 domain의 dataset인 news_contents.txt 으로 pretraining 한다\n",
        "### 새롭게 업데이트된 모델은 ./test-mlm 에 저장된다.\n",
        "!python run_mlm.py  --model_name_or_path 'kykim/bert-kor-base' --train_file news_contents.txt  --do_train --output_dir ./test-mlm --overwrite_output_dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-07-20 23:09:10.065968: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "07/20/2021 23:09:11 - WARNING - __main__ -   Process rank: -1, device: cpu, n_gpu: 0distributed training: False, 16-bits training: False\n",
            "07/20/2021 23:09:11 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=500,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=./test-mlm/runs/Jul20_23-09-11_b79942bfcbec,\n",
            "logging_first_step=False,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "output_dir=./test-mlm,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=8,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=test-mlm,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=None,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=./test-mlm,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "07/20/2021 23:09:11 - WARNING - datasets.builder -   Using custom data configuration default-938f4a6df542e53a\n",
            "Downloading and preparing dataset text/default (download: Unknown size, generated: Unknown size, post-processed: Unknown size, total: Unknown size) to /root/.cache/huggingface/datasets/text/default-938f4a6df542e53a/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5...\n",
            "Dataset text downloaded and prepared to /root/.cache/huggingface/datasets/text/default-938f4a6df542e53a/0.0.0/e16f44aa1b321ece1f87b07977cc5d70be93d69b20486d6dacd62e12cf25c9a5. Subsequent calls will reuse this data.\n",
            "[INFO|file_utils.py:1590] 2021-07-20 23:09:12,127 >> https://huggingface.co/kykim/bert-kor-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp11oqyqq4\n",
            "Downloading: 100% 725/725 [00:00<00:00, 545kB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-20 23:09:12,262 >> storing https://huggingface.co/kykim/bert-kor-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/4a55d5536891d7b55eaafd10464c67494d1ae106331c6c4accac769892617a49.17156685c3c033f09b54ffce87760d3303c9cec5500c742e0508068487a94752\n",
            "[INFO|file_utils.py:1602] 2021-07-20 23:09:12,262 >> creating metadata file for /root/.cache/huggingface/transformers/4a55d5536891d7b55eaafd10464c67494d1ae106331c6c4accac769892617a49.17156685c3c033f09b54ffce87760d3303c9cec5500c742e0508068487a94752\n",
            "[INFO|configuration_utils.py:530] 2021-07-20 23:09:12,262 >> loading configuration file https://huggingface.co/kykim/bert-kor-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4a55d5536891d7b55eaafd10464c67494d1ae106331c6c4accac769892617a49.17156685c3c033f09b54ffce87760d3303c9cec5500c742e0508068487a94752\n",
            "[INFO|configuration_utils.py:566] 2021-07-20 23:09:12,263 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"embedding_size\": 768,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 42000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1590] 2021-07-20 23:09:12,400 >> https://huggingface.co/kykim/bert-kor-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi7v0pvbc\n",
            "Downloading: 100% 80.0/80.0 [00:00<00:00, 56.9kB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-20 23:09:12,536 >> storing https://huggingface.co/kykim/bert-kor-base/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/52fec08ee92e3d2aef30c6c225f07d017383b14f89d36d2f7e97dad2f1e013d1.cbeb6d614d6fa18f12b887150236fbacf7aa47161d1292c3779477e43b12812a\n",
            "[INFO|file_utils.py:1602] 2021-07-20 23:09:12,537 >> creating metadata file for /root/.cache/huggingface/transformers/52fec08ee92e3d2aef30c6c225f07d017383b14f89d36d2f7e97dad2f1e013d1.cbeb6d614d6fa18f12b887150236fbacf7aa47161d1292c3779477e43b12812a\n",
            "[INFO|configuration_utils.py:530] 2021-07-20 23:09:12,674 >> loading configuration file https://huggingface.co/kykim/bert-kor-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/4a55d5536891d7b55eaafd10464c67494d1ae106331c6c4accac769892617a49.17156685c3c033f09b54ffce87760d3303c9cec5500c742e0508068487a94752\n",
            "[INFO|configuration_utils.py:566] 2021-07-20 23:09:12,674 >> Model config BertConfig {\n",
            "  \"architectures\": [\n",
            "    \"BertForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"directionality\": \"bidi\",\n",
            "  \"embedding_size\": 768,\n",
            "  \"gradient_checkpointing\": false,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"pooler_fc_size\": 768,\n",
            "  \"pooler_num_attention_heads\": 12,\n",
            "  \"pooler_num_fc_layers\": 3,\n",
            "  \"pooler_size_per_head\": 128,\n",
            "  \"pooler_type\": \"first_token_transform\",\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.8.2\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 42000\n",
            "}\n",
            "\n",
            "[INFO|file_utils.py:1590] 2021-07-20 23:09:12,809 >> https://huggingface.co/kykim/bert-kor-base/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpalx05_hg\n",
            "Downloading: 100% 344k/344k [00:00<00:00, 5.85MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-20 23:09:13,010 >> storing https://huggingface.co/kykim/bert-kor-base/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/6156f25e57e5f01249823906c3faca9dd502950551b0a1fee19a7f1ed30b3c41.8106bc2733ea9041a1bd1c717c63feb58e8291205b0ab7149d33ef57acbf7967\n",
            "[INFO|file_utils.py:1602] 2021-07-20 23:09:13,010 >> creating metadata file for /root/.cache/huggingface/transformers/6156f25e57e5f01249823906c3faca9dd502950551b0a1fee19a7f1ed30b3c41.8106bc2733ea9041a1bd1c717c63feb58e8291205b0ab7149d33ef57acbf7967\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-20 23:09:13,556 >> loading file https://huggingface.co/kykim/bert-kor-base/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/6156f25e57e5f01249823906c3faca9dd502950551b0a1fee19a7f1ed30b3c41.8106bc2733ea9041a1bd1c717c63feb58e8291205b0ab7149d33ef57acbf7967\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-20 23:09:13,556 >> loading file https://huggingface.co/kykim/bert-kor-base/resolve/main/tokenizer.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-20 23:09:13,557 >> loading file https://huggingface.co/kykim/bert-kor-base/resolve/main/added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-20 23:09:13,557 >> loading file https://huggingface.co/kykim/bert-kor-base/resolve/main/special_tokens_map.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:1717] 2021-07-20 23:09:13,557 >> loading file https://huggingface.co/kykim/bert-kor-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/52fec08ee92e3d2aef30c6c225f07d017383b14f89d36d2f7e97dad2f1e013d1.cbeb6d614d6fa18f12b887150236fbacf7aa47161d1292c3779477e43b12812a\n",
            "[INFO|file_utils.py:1590] 2021-07-20 23:09:13,778 >> https://huggingface.co/kykim/bert-kor-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0rcx9h6k\n",
            "Downloading: 100% 476M/476M [00:07<00:00, 64.8MB/s]\n",
            "[INFO|file_utils.py:1594] 2021-07-20 23:09:21,450 >> storing https://huggingface.co/kykim/bert-kor-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/0e8acf5d280808dcfcb46fac19ba5de95a1883cdc77e7f97649846d229c7eaaf.4deb9cfb433551b65f9f1a3712a7268e75a3e3660078f09ced865dc1d73c6520\n",
            "[INFO|file_utils.py:1602] 2021-07-20 23:09:21,450 >> creating metadata file for /root/.cache/huggingface/transformers/0e8acf5d280808dcfcb46fac19ba5de95a1883cdc77e7f97649846d229c7eaaf.4deb9cfb433551b65f9f1a3712a7268e75a3e3660078f09ced865dc1d73c6520\n",
            "[INFO|modeling_utils.py:1161] 2021-07-20 23:09:21,451 >> loading weights file https://huggingface.co/kykim/bert-kor-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/0e8acf5d280808dcfcb46fac19ba5de95a1883cdc77e7f97649846d229c7eaaf.4deb9cfb433551b65f9f1a3712a7268e75a3e3660078f09ced865dc1d73c6520\n",
            "[WARNING|modeling_utils.py:1337] 2021-07-20 23:09:24,594 >> Some weights of the model checkpoint at kykim/bert-kor-base were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "[INFO|modeling_utils.py:1354] 2021-07-20 23:09:24,594 >> All the weights of BertForMaskedLM were initialized from the model checkpoint at kykim/bert-kor-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n",
            "  8% 2/24 [00:00<00:01, 16.32ba/s][WARNING|tokenization_utils_base.py:3193] 2021-07-20 23:09:24,766 >> Token indices sequence length is longer than the specified maximum sequence length for this model (613 > 512). Running this sequence through the model will result in indexing errors\n",
            "100% 24/24 [00:01<00:00, 23.39ba/s]\n",
            "100% 24/24 [00:07<00:00,  3.33ba/s]\n",
            "[INFO|trainer.py:521] 2021-07-20 23:09:32,906 >> The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask.\n",
            "[INFO|trainer.py:1153] 2021-07-20 23:09:32,919 >> ***** Running training *****\n",
            "[INFO|trainer.py:1154] 2021-07-20 23:09:32,919 >>   Num examples = 1399\n",
            "[INFO|trainer.py:1155] 2021-07-20 23:09:32,919 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1156] 2021-07-20 23:09:32,919 >>   Instantaneous batch size per device = 8\n",
            "[INFO|trainer.py:1157] 2021-07-20 23:09:32,919 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
            "[INFO|trainer.py:1158] 2021-07-20 23:09:32,919 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1159] 2021-07-20 23:09:32,919 >>   Total optimization steps = 525\n",
            "{'loss': 1.7117, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}\n",
            " 95% 500/525 [50:11<02:28,  5.93s/it][INFO|trainer.py:1908] 2021-07-20 23:59:44,171 >> Saving model checkpoint to ./test-mlm/checkpoint-500\n",
            "[INFO|configuration_utils.py:364] 2021-07-20 23:59:44,172 >> Configuration saved in ./test-mlm/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:898] 2021-07-20 23:59:44,953 >> Model weights saved in ./test-mlm/checkpoint-500/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-20 23:59:44,953 >> tokenizer config file saved in ./test-mlm/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-20 23:59:44,953 >> Special tokens file saved in ./test-mlm/checkpoint-500/special_tokens_map.json\n",
            "100% 525/525 [52:46<00:00,  5.97s/it][INFO|trainer.py:1349] 2021-07-21 00:02:19,314 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 3166.3949, 'train_samples_per_second': 1.325, 'train_steps_per_second': 0.166, 'train_loss': 1.7043841480073474, 'epoch': 3.0}\n",
            "100% 525/525 [52:46<00:00,  6.03s/it]\n",
            "[INFO|trainer.py:1908] 2021-07-21 00:02:19,316 >> Saving model checkpoint to ./test-mlm\n",
            "[INFO|configuration_utils.py:364] 2021-07-21 00:02:19,317 >> Configuration saved in ./test-mlm/config.json\n",
            "[INFO|modeling_utils.py:898] 2021-07-21 00:02:20,135 >> Model weights saved in ./test-mlm/pytorch_model.bin\n",
            "[INFO|tokenization_utils_base.py:1948] 2021-07-21 00:02:20,135 >> tokenizer config file saved in ./test-mlm/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:1954] 2021-07-21 00:02:20,135 >> Special tokens file saved in ./test-mlm/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     1.7044\n",
            "  train_runtime            = 0:52:46.39\n",
            "  train_samples            =       1399\n",
            "  train_samples_per_second =      1.325\n",
            "  train_steps_per_second   =      0.166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMEOQk59kWIT",
        "outputId": "0bdc2b78-8ae9-4ab5-fc5e-ab50fb641668"
      },
      "source": [
        "### Huggingface에 새롭게 update된 나만이 모델을 업로드 한다. (Huggings 회원 가입 필요)\n",
        "\n",
        "!sudo apt-get install git-lfs\n",
        "\n",
        "!transformers-cli login\n",
        "!transformers-cli repo create esg-bert-kor\n",
        "\n",
        "!rm -rf /content/test-mlm/esg-bert-kor\n",
        "!git clone https://huggingface.co/jinbbong/esg-bert-kor\n",
        "\n",
        "%cd '/content/test-mlm'\n",
        "!ls\n",
        "\n",
        "!git init\n",
        "\n",
        "!git add all_results.json\n",
        "!git add config.json\n",
        "!git add pytorch_model.bin\n",
        "!git add special_tokens_map.json\n",
        "!git add tokenizer.json\n",
        "!git add tokenizer_config.json\n",
        "!git add train_results.json\n",
        "!git add trainer_state.json\n",
        "!git add training_args.bin\n",
        "!git add vocab.txt\n",
        "\n",
        "!git status\n",
        "\n",
        "!git config --global user.email \"swecomic@naver.com\"\n",
        "!git config --global user.name \"jinbbong\"\n",
        "\n",
        "!git commit -m \"Initial commit\"\n",
        "!git push --set-upstream origin master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[master 9b5f4f3] Initial commit\n",
            " 14 files changed, 42057 insertions(+)\n",
            " create mode 100644 checkpoint-500/config.json\n",
            " create mode 100644 checkpoint-500/optimizer.pt\n",
            " create mode 100644 checkpoint-500/pytorch_model.bin\n",
            " create mode 100644 checkpoint-500/rng_state.pth\n",
            " create mode 100644 checkpoint-500/scheduler.pt\n",
            " create mode 100644 checkpoint-500/special_tokens_map.json\n",
            " create mode 100644 checkpoint-500/tokenizer.json\n",
            " create mode 100644 checkpoint-500/tokenizer_config.json\n",
            " create mode 100644 checkpoint-500/trainer_state.json\n",
            " create mode 100644 checkpoint-500/training_args.bin\n",
            " create mode 100644 checkpoint-500/vocab.txt\n",
            " create mode 160000 esg-bert-kor\n",
            " create mode 100644 runs/Jul20_23-09-11_b79942bfcbec/1626822572.9342806/events.out.tfevents.1626822572.b79942bfcbec.676.1\n",
            " create mode 100644 runs/Jul20_23-09-11_b79942bfcbec/events.out.tfevents.1626822572.b79942bfcbec.676.0\n",
            "fatal: could not read Username for 'https://huggingface.co': No such device or address\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOwBf1iFmm1z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}